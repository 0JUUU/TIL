### 2. 빅데이터 처리단계

5단계로 빅데이터 처리❗ ⇒ 이를 통해 경쟁력 강화, 생산성 향상, 상황 분석, 미래 예측 등이 가능

- 내용

  수집

  - 데이터 수집
  - 정형, 비정형, 반정형 데이터 수집

  정제

  - 수집한 데이터를 적재하기 위해 필요 없는 데이터, 깨진 데이터 정리
  - 반정형, 비정형 데이터 : 분석에 필요 없는 부분 제거

  적재

  - 정제된 데이터를 분석하기 위해 적재하는 단계
  - RDB, NoSQL 데이터베이스, Redshift, Druid 등의 도구에 적재

  분석

  - 적재한 데이터를 의미 있는 지표로 분석하는 단계
  - 의사결정권자나 이용자가 사용할 수 있는 데이터로 분석하는 단계

  시각화

  - 분석한 것을 차트, 도표 등으로 보여주는 단계

    

    

  - **수집**

    > 참고 : https://velog.io/@dauuuum/빅데이터-데이터-수집

    내/외부에서 데이터 수집 ➡ 다양한 형태의 데이터 수집

    ➡ 내부 데이터 : 시스템 로그, DB 데이터

    외부 데이터 : 동영상, 오디오 정보 / 웹 크롤링 데이터 / SNS 데이터

    수집 방식

    HTTP 웹 서비스, RDB, FTP, JMS, Text, 이미지, 동영상, GPS, IoT 디바이스 센서 등

    수집 기술

    - Flume : 로그 데이터를 효율적으로 수집, 취합, 이동하기 위한 분산형 소프트웨어
    - Kafka : 오픈 소스 메시지 브로커 프로젝트
    - Sqoop : 관계형 데이터 베이스와 아파치 하둡 간의 대용량 데이터들을 효율적으로 변환하여 주는 명령 줄 인터페이스 애플리케이션
    - Nifi : 소프트웨어 시스템 간 데이터 흐름을 자동화하도록 설계된 소프트웨어 프로젝트
    - Flink : 오픈 소스 스트림 처리 프레임워크
    - Splunk : 기계가 생성한 빅데이터를 웹 스타일 인터페이스를 통해 검색, 모니터링, 분석하는 소프트웨어
    - Logstash : 실시간 파이프라인 기능을 가진 오픈소스 데이터 수집 엔진
    - Fluentd : 크로스 플랫폼 오픈 소스 데이터 수집 소프트웨어 프로젝트
    - 너치? scrapy ?

  - **정제**

    데이터를 분석 가능한 형태로 정리❗ (오류 데이터, 불필요한 데이터 제거 ➡ 정제된 데이터는 압축하여 데이터 사이즈 줄임)

    1. Identification
       - 알려진 다양한 데이터 포맷이나 비정형 데이터에 할당된 기본 포맷을 식별
    2. Filtration
       - 수집된 정보에서 정확하지 않은 데이터는 제외
    3. Validation
       - 데이터 유효성을 검증
    4. Noise Reduction
       - 오류 데이터를 제거
       - 분석 불가능한 데이터는 제외
    5. Transformation
       - 데이터를 분석 가능한 형태로 변환
    6. Compression
       - 저장장치 효율성을 위해 변환한 데이터를 압축
    7. Integration
       - 처리 완료한 데이터를 적재

  - **적재**

    대량의 데이터 ➡ 안전하게 보관 & 분석할 수 있는 환경으로 옮기는 과정

    분석 도구에 따라 NoSQL, RDB, 클라우드 스토리지, HDFS 등 다양한 환경으로 데이터 적재

    ❗ RDB 추출 데이터 or CSV 형태 데이터 : 별도의 정제 단계 없이 바로 적재 가능

  - **분석**

    적재된 데이터 활용 ➡  의사 결정을 위한 데이터를 제공하기 위한 리포트 생성

    (빠르게 분석 → 처리 엔진 / 효율적으로 분석 → 파티셔닝, 인덱싱)

  - **시각화**

    그래프 등 사용자가 빠르게 인식할 수 있는 형태로 시각화하는 과정